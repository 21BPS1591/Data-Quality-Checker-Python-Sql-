# SQL Data Quality Checker

## Overview

**SQL Data Quality Checker** is a comprehensive Python toolkit for automated data quality assessment and monitoring of SQLite databases. It provides a suite of configurable checks-including missing values, duplicates, invalid data, referential integrity, schema validation, freshness, and outlier detection-along with reporting and auto-fix capabilities. The toolkit is designed for integration into ETL pipelines, data engineering workflows, and automated alerting systems.

---

## Features

- **Automated Data Quality Checks**
  - Missing values
  - Duplicate detection
  - Invalid value checks (custom business rules)
  - Referential integrity (consistency across tables)
  - Schema validation against expected definitions
  - Data freshness (timeliness of updates)
  - Outlier detection in numeric columns

- **Reporting**
  - Generates reports in CSV, JSON, and HTML formats
  - Summarizes issues by severity and type
  - Includes sample problematic records

- **Auto-fix Capabilities**
  - Optionally removes duplicates and fills missing values based on configuration

- **Alerting**
  - Simulates alerting via email or console when critical issues are found

- **Visualization**
  - Plots trends of data quality issues over time and across different configuration profiles

- **Configurable Profiles**
  - Supports multiple configuration profiles for different data domains (default, financial, customer data, etc.)

---
## Visualization Examples

Below are sample visualizations generated by the toolkit:

### Data Quality Issues Over Time

![Data Quality Issues by Severity and Type Over Time](download-1.jpg)

*Left: Issues by severity over time. Right: Issues by type over time.*

### Comparison of Configuration Profiles

![Comparison of Different Configuration Profiles](download.jpg)

*Comparison of data quality issues across different configuration profiles.*

---
## Installation

This project requires Python 3 and the following libraries:
- pandas
- numpy
- matplotlib
- seaborn
- sqlite3 (standard library)
- smtplib, email (standard library)
- json, logging, io, time, os, datetime (standard library)

Install dependencies via pip if needed:
```bash
pip install pandas numpy matplotlib seaborn
```

---

## Usage

### 1. Creating a Sample Database

A helper function is provided to create a sample in-memory SQLite database with `customers`, `orders`, and `products` tables, including intentional data quality issues for demonstration.

```python
conn = create_sample_database()
```

### 2. Running Data Quality Checks

Instantiate the checker with a database connection and a configuration profile:

```python
checker = SQLDataQualityChecker(conn, config_profiles["default"])
issues = checker.run_all_checks()
```

### 3. Generating Reports

Generate reports in various formats:

```python
csv_report = checker.generate_report(format='csv')
json_report = checker.generate_report(format='json')
html_report = checker.generate_report(format='html')
```

### 4. Auto-fixing Issues

If enabled in the configuration, the checker can automatically remove duplicates and fill missing values:

```python
checker.auto_fix_issues()
```

### 5. Emailing Reports

Send the generated report via email (requires SMTP configuration):

```python
email_config = {
    'sender': 'your@email.com',
    'recipients': ['recipient@email.com'],
    'smtp_server': 'smtp.example.com',
    'smtp_port': 587,
    'username': 'your@email.com',
    'password': 'yourpassword',
    'subject': 'Data Quality Report'
}
checker.send_email_report(email_config)
```

---

## Configuration

Profiles are defined as Python dictionaries, allowing fine-grained control over:

- **Business Rules**: Custom SQL conditions for column validity
- **Relationships**: Foreign key integrity checks
- **Schema**: Expected table and column definitions
- **Freshness**: Maximum allowed data staleness
- **Outliers**: Detection method and thresholds
- **Auto-fixes**: Enable/disable and specify fix strategies

Example:
```python
config_profiles = {
    "default": {
        'business_rules': {
            'customers': {'age': 'age > 0 AND age  0'}
        },
        'relationships': [
            {'source_table': 'orders', 'source_column': 'customer_id', 'target_table': 'customers', 'target_column': 'id'}
        ],
        ...
    }
}
```

---

## Advanced Usage

### Simulating Scheduled Runs

Monitor data quality trends over time:

```python
simulate_scheduled_runs(conn, config_profiles["default"], days=7)
```

### Simulating ETL Pipelines

Integrate quality checks into ETL workflows:

```python
simulate_etl_pipeline(conn, config_profiles["default"])
```

### Alerting

Trigger alerts based on issue severity:

```python
simulate_alerting(issues, threshold=2)
```

### Comparing Configuration Profiles

Evaluate different quality requirements for various data domains:

```python
showcase_different_configurations(conn)
```

---

## Extending

- **Add new checks**: Extend the `SQLDataQualityChecker` class with custom methods.
- **Support more databases**: Adapt the SQL queries for other SQL dialects (e.g., PostgreSQL, MySQL).
- **Integrate with orchestration tools**: Use in Airflow, Prefect, or other workflow managers.

---

## Demo

Run the full demonstration (including database creation, checks, reporting, alerting, and visualization):

```bash
python your_script.py
```


